---
title: "Movielens capstone project"
author: "Daisuke Ohnuki"
date: "4/26/2021"
output:
  bookdown::pdf_document2:
    latex_engine: xelatex
  tufte::tufte_handout:
    latex_engine: xelatex
  '': default
---

\newpage

# Introduction
Recommendation system are among most important applications of machine learning deployed by digital companies including Amazon and Netflix today. They use the systems to understand their customers' tendencies to target them with their products in more effective and efficient way. For example, Netflix awarded a one million prize for data scientists who could successfully achieved the challenge for improving their recommendation algorithm by 10 per cent.

The MovieLens datasets have provided a popular environment for experiencing with machie learning since their launching in 1997.

The goal of this capstone project is to develop a recommendation system using the MovieLens datasets with ten million movie ratings, for achieving the RMSE, root means square error, of less than 0.86490. 

To facilitate this project, first we split the datasets into a training set, as "edx set", and final hold-out test set, as "validation set".
Second, we set out the exploratory data analysis with visualization techniques.
Then we build up the modelings to find an appropriate one, which we would apply to the validation set.
We conclude with our outcome for the final model, with limitations of this project and possibilities for future works. 


```{r , echo = FALSE, message=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(caret)
library(data.table)
library(knitr)
library(dplyr)
library(kableExtra)
```


# Exploratory Data Analysis
First of all, download the Movielens dataset and create Edx set and validation set from the Movielens dataset.
```{r exploratory data analysis, echo=TRUE, message=FALSE}
#Create Edx set and validation set from Movielens data set.
options(timeout=300)
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind = "Rounding") 
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")
# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```


Second, looking through the head data and the dimension of the edx data set.
The edx data set consists of 9,000,055 rows and 6 colums, with 10,677 unique movies and 69,878 unique users.
```{r check edx data sample,  echo=FALSE, message=FALSE}
head(edx)
```
```{r check edx dimension, echo=FALSE, message=FALSE}
# We can see edx train set (about 90% of original data) dimension
dim(edx)
```
Also, check the dimention of the train set.
```{r check users and movies, echo=FALSE, message=FALSE}
# We can see train set (about 90% of original data) dimension
edx %>% 
  summarize(n_users = n_distinct(userId),
            n_movies = n_distinct(movieId))
```
## Ratings
The average of rating is 3.51. The median is 4.
```{r - overall-ratings, echo=TRUE, message=FALSE}
# Plot distribution of ratings in the edx dataset
edx %>% ggplot(aes(rating)) +
  geom_histogram(binwidth = 0.3, color = I("black")) +
  scale_y_continuous(breaks = c(1000000, 2000000), labels = c("1", "2")) +
  labs(x = "Rating", y = "Count in million", caption = "edx dataset") + 
  ggtitle("Rating distribution") 
```
```{r, echo=FALSE}
mean(edx$rating)
median(edx$rating)
```

## Movies
As the following figure indicates, the average rating by movie concentrates around the total average. 
```{r - movie-effects-average, echo=TRUE, message=FALSE}
# Plot average rating by movie in the edx dataset
edx %>% group_by(movieId) %>%
  summarise(ave_rating = sum(rating)/n()) %>%
  ggplot(aes(ave_rating)) +
  geom_histogram(bins=30, color = I("black")) +
  labs(x = "Average rating", y = "Number of movies", caption = "edx dataset") +
  ggtitle("Movie distribution by average rating")

```
Plot the number of rating by movie.
```{r - movie-effects, echo=TRUE,message=FALSE}
edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram( bins=30, color = I("black"))+
  scale_x_log10() +
  labs(x = "Movies", y = "Number of ratings", caption = "edx dataset") + 
  ggtitle("Number of Ratings by movie")
```

## Users
We can see users' distribution.
```{r - user-effects}
# Plot average rating by user in the edx dataset
edx %>% group_by(userId) %>%
  summarise(ave_rating = sum(rating)/n()) %>%
  ggplot(aes(ave_rating)) +
  geom_histogram(bins=30, color = I("black")) +
  labs(x = "Average rating", y = "Number of users", caption = "edx dataset") + 
  ggtitle("User distribution by average rating")

```

Plot number of ratings by user in the edx dataset.
```{r - user-effects-numbers}
# Plot number of ratings by user in the edx dataset
edx %>% 
  count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram( bins=30, color = I("black")) +
  scale_x_log10() +
  labs(x = "Users", y = "Number of ratings", caption = "edx dataset") + 
  ggtitle("Number of ratings by User")
```

## Genre effect
We can also see the head of genres.
```{r check edx top genre list, echo=FALSE, message=FALSE}
# We can see some examples of top movie genre list in edx set
edx %>% group_by(genres) %>% 
  summarise(n=n()) %>%
  arrange(desc(n)) %>%
  head()
```
As the figure below shows, the average rating of genre combination varies.
Thus, we suspect genres effect ratings.
```{r - genre-effects}
# Plot average rating by genre for genre combinations with at least 50,000 ratings
edx %>% group_by(genres) %>%
  summarize(n = n(), avg = mean(rating), se = sd(rating)/sqrt(n())) %>%
  filter(n >= 50000) %>% 
  mutate(genres = reorder(genres, avg)) %>%
  ggplot(aes(x = genres, y = avg, ymin = avg - 2*se, ymax = avg + 2*se)) + 
  geom_point() +
  geom_errorbar() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "Genre combination", y = "Average Rating", caption = "edx dataset")+
  ggtitle("Average rating by Genre")
```



# Modeling and results
As we observed the effects of variables in the dataset, we focuse on Movies, UserID and genre as variables for rating and modeling with them.

Root mean Square Error, or RMSE is used to measure the differences between predicted values(y hat) as predicted rating of movie(i) by user(u) and observed values(y). If this number is larger than 1, it means our typical error is larger than 1 rating star. Which is not good. Which can write in code as below.
$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$


As the validation dataset was reserved for the final test, the edx dataset is necessary for both to train and test the algorithm in development. It would refrain the risk of over-training in cross-validation. 
```{r split edx data into train_set and test_set, echo=FALSE, message=FALSE}
# Create a training set (90% of edx) and a test_set (10% of edx data)
set.seed(1) 
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train_set <- edx[-test_index,]
temp <- edx[test_index,]
# Make sure userId and movieId in test set are also in train set
test_set <- temp %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
# Add rows removed from test set back into train set
removed <- anti_join(temp, test_set)
train_set <- rbind(train_set, removed)
rm(test_index, temp, removed)
```

Check the the average rating of the train set.
```{r RMSE function, echo=FALSE, message=FALSE}
#Here again check the the average rating of the train set.
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
mu <- mean(train_set$rating)
mu
```

Our first model is just the average model RMSE.
$$ Y_{u,i} =\mu + \epsilon_{u,i}$$
We obtain the following RMSE:

```{r naive_rmse, echo=TRUE, message=FALSE}
# Our first model formula is Yu,i = mu + Eu,i
# If we predict all unknown ratings with mu, we obtain the following RMSE:
options(pillar.sigfig = 5)
naive_RMSE <- RMSE(test_set$rating, mu)
naive_RMSE
```
As above, the result of this model is the average of rating equal 3.5125. The first model, naive model, is 1.06005.


## Movie Effect
Our second model: modeling movie effect formula will be adding movie bias as follow:
$$Y_{u,i} = \mu +b_{i}+ \epsilon_{u,i}$$
```{r movie effect qplot, echo=TRUE, message=FALSE}
# Our second model: modeling movie effect formula will be adding movie bias as follow: Yu,i = mu + b_i + Eu,i
mu <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))
```

```{r movie effect rmse, echo=TRUE, message=FALSE}
options(pillar.sigfig = 5)
predicted_ratings <- mu + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)
model2 <- RMSE(predicted_ratings, test_set$rating)
model2
```

The Movie effect model RMSE is 0.94296.




## Movie + User Effect
Our third one is movie plus user effect model.
 $$Y_{u,i} = \mu +b_{i}+b_{u}+\epsilon_{u,i}$$
```{r movie_user effect rmse, echo=TRUE, message=FALSE}
user_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarise(b_u = mean(rating - mu - b_i))
```
```{r ,echo=FALSE, message=FALSE}
options(pillar.sigfig = 5)
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)
model3 <- RMSE(predicted_ratings, test_set$rating)
model3
```


## Movie + User + Genre Effect
Our fourth model is movie, user and genre effected model.
 $$Y_{u,i} = \mu +b_{i}+b_{u} +b_{g}+\epsilon_{u,i}$$
```{r - plus-genre-effect-model, echo=TRUE, message=FALSE}
# Estimate genre effect (b_g)
options(pillar.sigfig = 5)
genre_avgs <- train_set %>%
  left_join(movie_avgs, by = "movieId") %>%
  left_join(user_avgs, by = "userId") %>%
  group_by(genres) %>%
  summarise(b_g = mean(rating - mu - b_i - b_u))
# Predict ratings adjusting for movie, user and genre effects
predicted_ratings <- test_set %>%
  left_join(movie_avgs, by = "movieId") %>%
  left_join(user_avgs, by = "userId") %>%
  left_join(genre_avgs, by = "genres") %>%
  mutate(pred = mu + b_i + b_u + b_g) %>%
  pull(pred)
# Calculate RMSE based on genre effects model
model4 <- RMSE(predicted_ratings, test_set$rating)
model4
```



## Regularized Movie + User + Genre Effect Model
For our fifth model, as the final model, we are going to add regularization to improve our predicted RMSE.
We can use regularization for the estimate movie + user effects as well. 
We are minimizing:
$$\frac{1}{N}\sum_{u,i}\left(y_{u,i}-\mu-b_i-b_u-b_g\right)^2+\lambda\left(\sum_ib_i^2+\sum_ub_u^2+\sum_gb_g^2\right)$$ 
Our final  model: Regularized Movie Effect + User Effect + Genre Effect Model
$$Y_{u,i} = \mu +b_{i}+b_{u} +b_{g}+\epsilon_{u,i}$$
Choosing the best penalty terms. Lambda is a tuning parameter. 
We can use cross-validation to choose it to get our optimized lambda.
Here is the plot of lambda by increment of 0.25 from 0 to 10, and optimized lambda value.

```{r lambda, echo=TRUE, message=FALSE}
set.seed(1)
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  
  mu <- mean(train_set$rating)
  
  b_i <- train_set %>% 
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - b_i - mu)/(n()+l))
  
  b_g <- train_set %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    group_by(genres) %>%
    summarise(b_g = sum(rating - b_i - b_u - mu)/(n()+l))
  
  predicted_ratings <- test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "genres") %>%
    mutate(pred = mu + b_i + b_u + b_g) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses)+ggtitle("Tuning parameter:lambda")
lambda <- lambdas[which.min(rmses)]
lambda
```
## Regularized Movie + User + Genre Effect model to the Validation set. 
Apply the regularized final model with the lambda of [lambda] to the validation set.
```{r final model on validation, echo=TRUE, message=FALSE}
set.seed(1)
options(pillar.sigfig = 5)
lambda <- lambda
mu <- mean(edx$rating)
b_i <- edx %>% 
  group_by(movieId) %>%
  summarise(b_i = sum(rating - mu)/(n()+lambda))
b_u <- edx %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarise(b_u = sum(rating - b_i - mu)/(n()+lambda))
b_g <- train_set %>%
  left_join(b_i, by="movieId") %>%
  left_join(b_u, by="userId") %>%
  group_by(genres) %>%
  summarise(b_g = sum(rating - b_i - b_u - mu)/(n()+lambda))
predicted_ratings <- validation %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(b_g, by = "genres") %>%
  mutate(pred = mu + b_i + b_u + b_g) %>%
  pull(pred)
finalmodel_validation_rmse <- RMSE(predicted_ratings, validation$rating)
finalmodel_validation_rmse

```

Overall results are as follows:
```{r rmse_results goal, echo=TRUE, message=FALSE}
options(pillar.sigfig = 5)
rmse_results <- data.frame(method = "RMSE Goal", RMSE = 0.86490)
finalmodel_validation_rmse <- bind_rows(rmse_results, 
                          data.frame(method = "Regularized Movie ï¼‹ User + Genre model on Validation set", 
                                 RMSE = finalmodel_validation_rmse))
finalmodel_validation_rmse
```

# Conclusion
We achieved RMSE less than 0.86490 with our final model, the regularized Movie, User and Genre effected model in the validation set.
The limitation of this project is derived from that we did not calculate the inter-variable contribution for the modeling. For future work, for example,  we should find out inter variation correlation on how each variable contribute and effect. 

# Reference
Irizarry A. Rafael (2018) Introduction to Data Science: Data Analysis and Prediction Algorithms with R.
